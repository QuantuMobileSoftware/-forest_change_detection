{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c529015a",
   "metadata": {},
   "source": [
    "### **For given aoi, run Deforestation model(Unet-Diff) inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed86eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import urllib\n",
    "import uuid\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import geojson\n",
    "import geopandas\n",
    "import geopandas as gp\n",
    "import geopandas as gpd\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import segmentation_models_pytorch as smp\n",
    "import shapely\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from geojson import Feature\n",
    "from geopandas import GeoSeries\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from rasterio import Affine, features\n",
    "from rasterio.mask import mask as riomask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import reshape_as_image, reshape_as_raster\n",
    "from rasterio.warp import Resampling, calculate_default_transform, reproject\n",
    "from rasterio.windows import Window\n",
    "from scipy import spatial\n",
    "from sentinel2download.downloader import Sentinel2Downloader, logger\n",
    "from sentinel2download.overlap import Sentinel2Overlap\n",
    "from shapely import wkt\n",
    "from shapely.geometry import MultiPolygon, Polygon, box\n",
    "from shapely.ops import transform, unary_union\n",
    "from skimage.exposure import match_histograms\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Functions to create, load the model(Use them instead of Catalyst lib code)\n",
    "def prepare_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def prepare_model(model):\n",
    "    device = prepare_device()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        raise ValueError(\"Multi GPU mode is not supported\")\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def load_model(network, model_weights_path):\n",
    "    if network == \"unet-diff\":\n",
    "        aux_params = dict(\n",
    "            pooling=\"max\",  # one of 'avg', 'max'\n",
    "            dropout=0.1,  # dropout ratio, default is None\n",
    "            activation=\"sigmoid\",  # activation function, default is None\n",
    "            classes=1,  # define number of output labels\n",
    "        )\n",
    "        model = smp.Unet(\n",
    "            \"resnet18\",\n",
    "            aux_params=aux_params,\n",
    "            encoder_weights=None,\n",
    "            in_channels=27,\n",
    "            encoder_depth=2,\n",
    "            decoder_channels=(256, 128),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown network\")\n",
    "\n",
    "    model, device = prepare_model(model)\n",
    "    model.load_state_dict(\n",
    "        torch.load(model_weights_path, map_location=torch.device(device))\n",
    "    )\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceca116",
   "metadata": {},
   "source": [
    "### 0. Setting up parameters\n",
    "#### Read Input from environment and setup folders and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60529d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_ID = os.getenv(\"REQUEST_ID\")\n",
    "START_DATE = os.getenv(\"START_DATE\")\n",
    "END_DATE = os.getenv(\"END_DATE\")\n",
    "AOI = os.getenv(\"AOI\")\n",
    "SENTINEL2_GOOGLE_API_KEY = os.getenv(\"SENTINEL2_GOOGLE_API_KEY\")\n",
    "SATELLITE_CACHE_FOLDER = os.getenv(\"SENTINEL2_CACHE\")\n",
    "\n",
    "INPUT_FOLDER = os.path.dirname(SATELLITE_CACHE_FOLDER)\n",
    "OUTPUT_FOLDER = os.getenv(\"OUTPUT_FOLDER\")\n",
    "PREPARED_DATA_FOLDER = os.path.join(INPUT_FOLDER, \"prepared\")\n",
    "CODE_FOLDER = \"/code\"\n",
    "\n",
    "LANDCOVER_POLYGONS_PATH = os.path.join(CODE_FOLDER, \"data\", \"landcovers\")\n",
    "LANDCOVER_FILENAME = (\n",
    "    \"S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml\"\n",
    ")\n",
    "SENTINEL_TILES = os.path.join(LANDCOVER_POLYGONS_PATH, LANDCOVER_FILENAME)\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive.file\"]\n",
    "\n",
    "CLOUD_DATA_FOLDER = os.path.join(CODE_FOLDER, \"data\", \"clouds\")\n",
    "MODEL_PATH = os.path.join(CODE_FOLDER, \"models\", \"unet_diff.pth\")\n",
    "\n",
    "LOAD_DIR = SATELLITE_CACHE_FOLDER\n",
    "\n",
    "PRODUCT_TYPE = \"L1C\"\n",
    "BANDS = {\"TCI\", \"B01\", \"B02\", \"B04\", \"B05\", \"B08\", \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\"}\n",
    "CONSTRAINTS = {\n",
    "    \"NODATA_PIXEL_PERCENTAGE\": 1,\n",
    "    \"CLOUDY_PIXEL_PERCENTAGE\": 15.0,\n",
    "}\n",
    "CLOUDS_PROBABILITY_THRESHOLD = 1\n",
    "REMOVE_OTHER_DATES = True\n",
    "\n",
    "\n",
    "MAX_SHIFT_ITERS = 2\n",
    "MAX_SHIFT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf2ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(LANDCOVER_POLYGONS_PATH, exist_ok=True)\n",
    "os.makedirs(PREPARED_DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(CLOUD_DATA_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7393e45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020-05-01', '2020-06-30')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE, END_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e125cce",
   "metadata": {},
   "source": [
    "### 1. Transform AOI to GeoJSON file and then download data from Sentinel L1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d996771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = gp.GeoDataFrame(geometry=[wkt.loads(AOI)], crs=\"epsg:4326\")\n",
    "aoi_filename = \"provided_aoi.geojson\"\n",
    "aoi.to_file(aoi_filename, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494ed325",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2overlap = Sentinel2Overlap(aoi_path=aoi_filename)\n",
    "overlap_tiles = s2overlap.overlap_with_geometry()\n",
    "landcover_tiles = set(overlap_tiles.Name.apply(lambda x: x[:3]).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533905b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_date(date, delta=5, format=\"%Y-%m-%d\"):\n",
    "    date = datetime.strptime(date, format)\n",
    "    date = date - timedelta(days=delta)\n",
    "    return datetime.strftime(date, format)\n",
    "\n",
    "\n",
    "def diff_date(date_a, date_b, format=\"%Y-%m-%d\"):\n",
    "    date_a, date_b = datetime.strptime(date_a, format), datetime.strptime(\n",
    "        date_b, format\n",
    "    )\n",
    "    delta = date_a - date_b\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21993ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_timestamp, end_timestamp = datetime.strptime(\n",
    "    START_DATE, \"%Y-%m-%d\"\n",
    "), datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
    "year = ((end_timestamp - start_timestamp) / 2 + start_timestamp).year\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77c7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(tiles, start_date, end_date):\n",
    "    loader = Sentinel2Downloader(SENTINEL2_GOOGLE_API_KEY)\n",
    "    loadings = dict()\n",
    "\n",
    "    for tile in tiles:\n",
    "        start = start_date\n",
    "        end = end_date\n",
    "\n",
    "        print(f\"Loading images for tile: {tile}...\")\n",
    "        count = 0\n",
    "        while count < MAX_SHIFT_ITERS:\n",
    "            loaded = loader.download(\n",
    "                PRODUCT_TYPE,\n",
    "                [tile],\n",
    "                start_date=start,\n",
    "                end_date=end,\n",
    "                output_dir=LOAD_DIR,\n",
    "                bands=BANDS,\n",
    "                constraints=CONSTRAINTS,\n",
    "            )\n",
    "\n",
    "            if not loaded:\n",
    "                end = start_date\n",
    "                start = shift_date(start_date, delta=MAX_SHIFT)\n",
    "                print(\n",
    "                    f\"For tile: {tile} and dates {start_date} {end_date} proper images not found! Shift dates to {start} {end}!\"\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "        if loaded:\n",
    "            loadings[tile] = loaded\n",
    "            print(f\"Loading images for tile {tile} finished\")\n",
    "        else:\n",
    "            print(f\"Images for tile {tile} were not loaded!\")\n",
    "\n",
    "    # tile_folders = dict()\n",
    "    # for tile, tile_paths in loadings.items():\n",
    "    #    tile_folders[tile] = {str(Path(tile_path[0]).parent) for tile_path in tile_paths}\n",
    "    return loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95bcfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for tile: 16TFK...\n",
      "Loading images for tile 16TFK finished\n",
      "Loading images for tile: 16TFK...\n",
      "For tile: 16TFK and dates 2020-06-20 2020-06-30 proper images not found! Shift dates to 2020-05-21 2020-06-20!\n",
      "Loading images for tile 16TFK finished\n"
     ]
    }
   ],
   "source": [
    "loadings_start_date = load_images(\n",
    "    overlap_tiles.Name.values, shift_date(START_DATE, delta=10), START_DATE\n",
    ")\n",
    "loadings_end_date = load_images(\n",
    "    overlap_tiles.Name.values, shift_date(END_DATE, delta=10), END_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ffcd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_date(loadings, func=max, filtered=dict(), tag=\"start\"):\n",
    "    def _find_agg_date(folders, func=func):\n",
    "        dates = list()\n",
    "        for i, folder in enumerate(folders):\n",
    "            search = re.search(r\"_(\\d+)T\\d+_\", str(folder))\n",
    "            date = search.group(1)\n",
    "            date = datetime.strptime(date, \"%Y%m%d\")\n",
    "            dates.append(date)\n",
    "        last_date = func(dates)\n",
    "        last_date = datetime.strftime(last_date, \"%Y%m%d\")\n",
    "        return last_date\n",
    "\n",
    "    def _get_folder(files):\n",
    "        return os.path.join(\"/\", *files[0][0].split(\"/\")[:-1])\n",
    "\n",
    "    for tile, items in loadings.items():\n",
    "        try:\n",
    "            last_date = _find_agg_date(items)\n",
    "            bands_paths = dict()\n",
    "            for path, _ in items:\n",
    "                if PRODUCT_TYPE == \"L2A\":\n",
    "                    if last_date in path:\n",
    "                        if \"B8A_20m.jp2\" in path:\n",
    "                            bands_paths[\"B8A\"] = path\n",
    "                        if \"B11_20m.jp2\" in path:\n",
    "                            bands_paths[\"B11\"] = path\n",
    "                        if \"B04_10m.jp2\" in path:\n",
    "                            bands_paths[\"B04\"] = path\n",
    "                        if \"B08_10m.jp2\" in path:\n",
    "                            bands_paths[\"B08\"] = path\n",
    "                        if \"B12_20m.jp2\" in path:\n",
    "                            bands_paths[\"B12\"] = path\n",
    "                        if \"TCI_10m.jp2\" in path:\n",
    "                            bands_paths[\"TCI\"] = path\n",
    "                        folder = _get_folder(items)\n",
    "                elif PRODUCT_TYPE == \"L1C\":\n",
    "                    if last_date in path:\n",
    "                        if \"B8A.jp2\" in path:\n",
    "                            bands_paths[\"B8A\"] = path\n",
    "                        if \"B11.jp2\" in path:\n",
    "                            bands_paths[\"B11\"] = path\n",
    "                        if \"B04.jp2\" in path:\n",
    "                            bands_paths[\"B04\"] = path\n",
    "                        if \"B08.jp2\" in path:\n",
    "                            bands_paths[\"B08\"] = path\n",
    "                        if \"B12.jp2\" in path:\n",
    "                            bands_paths[\"B12\"] = path\n",
    "                        if \"TCI.jp2\" in path:\n",
    "                            bands_paths[\"TCI\"] = path\n",
    "                        folder_path = _get_folder(items)\n",
    "\n",
    "            info_dict = {\n",
    "                tag: dict(paths=bands_paths, date=last_date, folder=folder_path)\n",
    "            }\n",
    "\n",
    "            if tile in filtered.keys():\n",
    "                filtered[tile].update(info_dict)\n",
    "            else:\n",
    "                filtered.update({tile: info_dict})\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error for {tile}: {str(ex)}\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f94524a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered = filter_by_date(loadings_start_date, func=max, tag=\"start\")\n",
    "filtered = filter_by_date(loadings_end_date, func=max, filtered=filtered, tag=\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5e49cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711',\n",
       " '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tile_and_images_folders(fitered, idx=0):\n",
    "    tile = sorted(list(filtered.keys()))[0]\n",
    "    return tile, filtered[tile][\"start\"][\"folder\"], filtered[tile][\"end\"][\"folder\"]\n",
    "\n",
    "\n",
    "tile, start_date_folder, end_date_folder = get_tile_and_images_folders(filtered)\n",
    "start_date_folder, end_date_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7806bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def remove_not_used_dates(\n",
    "    start_date_folder, end_date_folder, cache_dir=SATELLITE_CACHE_FOLDER\n",
    "):\n",
    "    start_split, end_split = start_date_folder.split(\"/\"), end_date_folder.split(\"/\")\n",
    "    if cache_dir != os.path.join(\"/\", *start_split[:-1]) or cache_dir != os.path.join(\n",
    "        \"/\", *end_split[:-1]\n",
    "    ):\n",
    "        raise ValueError(\"cache_dir is not valid\")\n",
    "    used_dates = start_split[-1], end_split[-1]\n",
    "    for folder in os.listdir(cache_dir):\n",
    "        if folder not in used_dates:\n",
    "            shutil.rmtree(os.path.join(cache_dir, folder))\n",
    "\n",
    "\n",
    "# if REMOVE_OTHER_DATES:\n",
    "#     remove_not_used_dates(start_date_folder, end_date_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc3036",
   "metadata": {},
   "source": [
    "### 2. Preparing images (calculating ndmi ndvi, scaling, merging to tiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27292af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time_dependent.data_prepare.prepare_tif import (\n",
    "    get_ndmi,\n",
    "    get_ndvi,\n",
    "    merge,\n",
    "    scale_img,\n",
    "    search_band,\n",
    "    to_tiff,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_data(data_folder, save_path):\n",
    "    img_folder = data_folder\n",
    "\n",
    "    tmp_file = data_folder.split(\"/\")[-1]\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_file_merged = join(save_path, f\"all_merged_{tmp_file}.tif\")\n",
    "\n",
    "    bands, band_names = [\"TCI\", \"B08\", \"B8A\", \"B11\", \"B12\"], []\n",
    "\n",
    "    for band in bands:\n",
    "        band_names.append(join(img_folder, search_band(band, img_folder, \"jp2\")))\n",
    "\n",
    "    b4_name = join(img_folder, search_band(\"B04\", img_folder, \"jp2\"))\n",
    "    ndvi_name = join(img_folder, \"ndvi\")\n",
    "    ndmi_name = join(img_folder, \"ndmi\")\n",
    "    print(\"\\nall bands are converting to *tif...\\n\")\n",
    "\n",
    "    for band_name in band_names:\n",
    "        print(band_name[-3:])\n",
    "        if \"B08\" in band_name:\n",
    "            b8_name = band_name\n",
    "        if \"B8A\" in band_name:\n",
    "            b8a_name = band_name\n",
    "        if \"B11\" in band_name:\n",
    "            b11_name = band_name\n",
    "        to_tiff(f\"{band_name}.jp2\")\n",
    "\n",
    "    to_tiff(f\"{b4_name}.jp2\")\n",
    "    print(\"\\nndvi band is processing...\")\n",
    "\n",
    "    get_ndvi(f\"{b4_name}.tif\", f\"{b8_name}.tif\", f\"{ndvi_name}.tif\")\n",
    "\n",
    "    print(\"\\nndmi band is processing...\")\n",
    "\n",
    "    get_ndmi(f\"{b11_name}.tif\", f\"{b8a_name}.tif\", f\"{ndmi_name}.tif\")\n",
    "\n",
    "    band_names.append(ndvi_name)\n",
    "    band_names.append(ndmi_name)\n",
    "\n",
    "    bands.append(\"ndvi\")\n",
    "    bands.append(\"ndmi\")\n",
    "\n",
    "    print(\"\\nall bands are scaling to 8-bit images...\\n\")\n",
    "    band_names_scaled = []\n",
    "    for band_name in band_names:\n",
    "        print(band_name)\n",
    "        scaled_name = scale_img(f\"{band_name}.tif\")\n",
    "        band_names_scaled.append(scaled_name)\n",
    "\n",
    "    print(\"\\nall bands are being merged...\\n\")\n",
    "    print(band_names_scaled)\n",
    "\n",
    "    merge(save_file_merged, *band_names_scaled)\n",
    "\n",
    "    for item in os.listdir(img_folder):\n",
    "        if item.endswith(\".tif\"):\n",
    "            os.remove(join(img_folder, item))\n",
    "    return save_file_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f418c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all bands are converting to *tif...\n",
      "\n",
      "TCI\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B08\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B8A\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B11\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B12\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "ndvi band is processing...\n",
      "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
      "\n",
      "ndmi band is processing...\n",
      "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
      "\n",
      "all bands are scaling to 8-bit images...\n",
      "\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_TCI\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B08\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B8A\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B11\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B12\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/ndvi\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/ndmi\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "all bands are being merged...\n",
      "\n",
      "['/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_TCI_scaled', '/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B08_scaled', '/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B8A_scaled', '/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B11_scaled', '/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B12_scaled', '/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/ndvi_scaled', '/input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/ndmi_scaled']\n",
      "\n",
      "Processing file     1 of     7,  0.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_TCI_scaled.tif\n",
      "File Size: 10980x10980x3\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     2 of     7, 14.286% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B08_scaled.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     3 of     7, 28.571% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B8A_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     4 of     7, 42.857% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B11_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     5 of     7, 57.143% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B12_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     6 of     7, 71.429% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/ndvi_scaled.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     7 of     7, 85.714% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/ndmi_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "all bands are converting to *tif...\n",
      "\n",
      "TCI\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B08\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B8A\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B11\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B12\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "ndvi band is processing...\n",
      "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
      "\n",
      "ndmi band is processing...\n",
      "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
      "\n",
      "all bands are scaling to 8-bit images...\n",
      "\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_TCI\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B08\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B8A\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B11\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B12\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/ndvi\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/ndmi\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "all bands are being merged...\n",
      "\n",
      "['/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_TCI_scaled', '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B08_scaled', '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B8A_scaled', '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B11_scaled', '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B12_scaled', '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/ndvi_scaled', '/input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/ndmi_scaled']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file     1 of     7,  0.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_TCI_scaled.tif\n",
      "File Size: 10980x10980x3\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     2 of     7, 14.286% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B08_scaled.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     3 of     7, 28.571% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B8A_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     4 of     7, 42.857% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B11_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     5 of     7, 57.143% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B12_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     6 of     7, 71.429% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/ndvi_scaled.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,10980,10980.\n",
      "\n",
      "Processing file     7 of     7, 85.714% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/ndmi_scaled.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,10980,10980.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(PREPARED_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "start_date_merged_path = prepare_data(start_date_folder, PREPARED_DATA_FOLDER)\n",
    "end_date_merged_path = prepare_data(end_date_folder, PREPARED_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc0194",
   "metadata": {},
   "source": [
    "### 3.1 Prepare clouds tif files for postprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10c6c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_dependent.data_prepare.prepare_clouds import (\n",
    "    detect_clouds,\n",
    "    merge,\n",
    "    search_band,\n",
    "    to_tiff,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_clouds(data_folder, save_path):\n",
    "    img_folder = data_folder\n",
    "    tile_folder = data_folder.split(\"/\")[-1]\n",
    "    print(tile_folder)\n",
    "    bands, band_names = [\n",
    "        \"B01\",\n",
    "        \"B02\",\n",
    "        \"B04\",\n",
    "        \"B05\",\n",
    "        \"B08\",\n",
    "        \"B8A\",\n",
    "        \"B09\",\n",
    "        \"B10\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ], []\n",
    "\n",
    "    for band in bands:\n",
    "        band_names.append(join(img_folder, search_band(band, img_folder, \"jp2\")))\n",
    "\n",
    "    print(\"\\nall bands are converting to *tif...\\n\")\n",
    "\n",
    "    for band_name in band_names:\n",
    "        print(band_name[-3:])\n",
    "        to_tiff(f\"{band_name}.jp2\")\n",
    "\n",
    "    print(\"\\n all bands are being merged...\\n\")\n",
    "\n",
    "    save_file_merged = join(save_path, f\"{tile_folder}_full_merged.tif\")\n",
    "    merge(save_file_merged, *band_names)\n",
    "\n",
    "    save_file_clouds = join(save_path, f\"{tile_folder}_clouds.tiff\")\n",
    "    detect_clouds(save_file_merged, save_file_clouds)\n",
    "    os.remove(save_file_merged)\n",
    "\n",
    "    for item in os.listdir(img_folder):\n",
    "        if item.endswith(\".tif\"):\n",
    "            os.remove(join(img_folder, item))\n",
    "\n",
    "    # os.system(f'rm {join(granule_folder, tile_folder, 'IMG_DATA')}*.jp2')\n",
    "    print(\"\\ntemp files have been deleted\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c0ab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711\n",
      "\n",
      "all bands are converting to *tif...\n",
      "\n",
      "B01\n",
      "Input file size is 1830, 1830\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B02\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B04\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B05\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B08\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B8A\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B09\n",
      "Input file size is 1830, 1830\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B10\n",
      "Input file size is 1830, 1830\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B11\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B12\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      " all bands are being merged...\n",
      "\n",
      "\n",
      "Processing file     1 of    10,  0.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B01.tif\n",
      "File Size: 1830x1830x1\n",
      "Pixel Size: 60.000000 x -60.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,1830,1830 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     2 of    10, 10.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B02.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     3 of    10, 20.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B04.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     4 of    10, 30.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B05.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     5 of    10, 40.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B08.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     6 of    10, 50.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B8A.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     7 of    10, 60.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B09.tif\n",
      "File Size: 1830x1830x1\n",
      "Pixel Size: 60.000000 x -60.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,1830,1830 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     8 of    10, 70.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B10.tif\n",
      "File Size: 1830x1830x1\n",
      "Pixel Size: 60.000000 x -60.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,1830,1830 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     9 of    10, 80.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B11.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "\n",
      "Processing file    10 of    10, 90.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711/T16TFK_20200424T161829_B12.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "================================\n",
      "Cloud detection.\n",
      "predict.\n",
      "resize.\n",
      "save cloud.\n",
      "\n",
      "temp files have been deleted\n",
      "\n",
      "S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748\n",
      "\n",
      "all bands are converting to *tif...\n",
      "\n",
      "B01\n",
      "Input file size is 1830, 1830\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B02\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B04\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B05\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B08\n",
      "Input file size is 10980, 10980\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B8A\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B09\n",
      "Input file size is 1830, 1830\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B10\n",
      "Input file size is 1830, 1830\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B11\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "B12\n",
      "Input file size is 5490, 5490\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      " all bands are being merged...\n",
      "\n",
      "\n",
      "Processing file     1 of    10,  0.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B01.tif\n",
      "File Size: 1830x1830x1\n",
      "Pixel Size: 60.000000 x -60.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,1830,1830 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     2 of    10, 10.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B02.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     3 of    10, 20.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B04.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     4 of    10, 30.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B05.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     5 of    10, 40.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B08.tif\n",
      "File Size: 10980x10980x1\n",
      "Pixel Size: 10.000000 x -10.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,10980,10980 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     6 of    10, 50.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B8A.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     7 of    10, 60.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B09.tif\n",
      "File Size: 1830x1830x1\n",
      "Pixel Size: 60.000000 x -60.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,1830,1830 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     8 of    10, 70.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B10.tif\n",
      "File Size: 1830x1830x1\n",
      "Pixel Size: 60.000000 x -60.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,1830,1830 to 0,0,2745,2745.\n",
      "\n",
      "Processing file     9 of    10, 80.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B11.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "\n",
      "Processing file    10 of    10, 90.000% completed in 0 minutes.\n",
      "Filename: /input/SENTINEL2_CACHE/S2A_MSIL1C_20200611T162901_N0209_R083_T16TFK_20200611T200748/T16TFK_20200611T162901_B12.tif\n",
      "File Size: 5490x5490x1\n",
      "Pixel Size: 20.000000 x -20.000000\n",
      "UL:(600000.000000,4500000.000000)   LR:(709800.000000,4390200.000000)\n",
      "Copy 0,0,5490,5490 to 0,0,2745,2745.\n",
      "================================\n",
      "Cloud detection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict.\n",
      "resize.\n",
      "save cloud.\n",
      "\n",
      "temp files have been deleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(CLOUD_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "prepare_clouds(start_date_folder, CLOUD_DATA_FOLDER)\n",
    "prepare_clouds(end_date_folder, CLOUD_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20af44",
   "metadata": {},
   "source": [
    "### 3.2 Preparing forest landcover data, also needed for postprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f9247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandcoverPolygons:\n",
    "    \"\"\"\n",
    "    LandcoverPolygon class to access forest polygons. Before usage,\n",
    "    be sure that SENTINEL_TILES file is downloaded.\n",
    "    SENTINEL_TILES_POLYGONS = 'https://sentinel.esa.int/documents/247904/1955685/S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml'\n",
    "\n",
    "    :param tile: tile name (str), e.g. '36UYA'\n",
    "    :param crs: coordinate system (str), e.g. 'EPSG:4326'\n",
    "\n",
    "    :return polygons: list of forest polygons within a tile in CRS of a S2A image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tile, crs, year, aoi):\n",
    "        self.tile = tile\n",
    "        self.crs = crs\n",
    "        self.LANDCOVER_GEOJSON = prepare_landcover(\n",
    "            year, [tile[:3]], LANDCOVER_POLYGONS_PATH, aoi\n",
    "        )\n",
    "        gpd.io.file.fiona.drvsupport.supported_drivers[\"KML\"] = \"rw\"\n",
    "\n",
    "    def get_polygon(self):\n",
    "        polygon_path = os.path.join(LANDCOVER_POLYGONS_PATH, f\"{self.tile}.geojson\")\n",
    "        logging.info(f\"LANDCOVER_POLYGONS_PATH: {polygon_path}\")\n",
    "        if os.path.isfile(polygon_path):\n",
    "            logging.info(f\"{self.tile} forests polygons file exists.\")\n",
    "            polygons = gpd.read_file(polygon_path)\n",
    "        else:\n",
    "            logging.info(\n",
    "                f\"{self.tile} forests polygons file does not exist. Creating polygons...\"\n",
    "            )\n",
    "            polygons = self.create_polygon()\n",
    "\n",
    "        if len(polygons) > 0:\n",
    "            polygons = polygons.to_crs(self.crs)\n",
    "            polygons = list(polygons[\"geometry\"])\n",
    "        else:\n",
    "            logging.info(\"No forests polygons.\")\n",
    "        return polygons\n",
    "\n",
    "    def create_polygon(self):\n",
    "        polygons = []\n",
    "        if os.path.isfile(SENTINEL_TILES):\n",
    "            logging.info(\n",
    "                f\"read forests_polygons_file: {SENTINEL_TILES}, for tile {self.tile}\"\n",
    "            )\n",
    "\n",
    "            sentinel_tiles = gpd.read_file(SENTINEL_TILES, driver=\"KML\")\n",
    "            sentinel_tiles = sentinel_tiles[sentinel_tiles[\"Name\"] == self.tile]\n",
    "\n",
    "            logging.info(f\"sentinel_tiles for {self.tile}: {sentinel_tiles}\")\n",
    "\n",
    "            bounding_polygon = sentinel_tiles[\"geometry\"].values[0]\n",
    "            polygons = gpd.read_file(self.LANDCOVER_GEOJSON)\n",
    "            polygons = polygons[polygons[\"geometry\"].intersects(bounding_polygon)]\n",
    "            polygon_path = os.path.join(LANDCOVER_POLYGONS_PATH, f\"{self.tile}.geojson\")\n",
    "\n",
    "            logging.info(f\"forests_polygons_file_path: {polygon_path}\")\n",
    "\n",
    "            if polygons.empty:\n",
    "                return polygons\n",
    "            polygons.to_file(polygon_path, driver=\"GeoJSON\")\n",
    "        else:\n",
    "            logging.error(f\"{SENTINEL_TILES} doth not exists\")\n",
    "            raise FileNotFoundError\n",
    "        return polygons\n",
    "\n",
    "\n",
    "def landcover_annual(year, landcover_tiles, output_path, aoi):\n",
    "    # landcover_classes = {\n",
    "    #    1: \"Water\",\n",
    "    #    2: \"Trees\",\n",
    "    #    4: \"Flooded vegetation\",\n",
    "    #    5: \"Crops\",\n",
    "    #    7: \"Built Area\",\n",
    "    #    8: \"Bare ground\",\n",
    "    #   9: \"Snow/Ice\",\n",
    "    #   10: \"Clouds\",\n",
    "    #    11: \"Rangeland\"\n",
    "    # }\n",
    "    EPSG = \"EPSG:4326\"\n",
    "    landcover_downloaded = []\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for tile_i in landcover_tiles:\n",
    "        tile_url = f\"https://lulctimeseries.blob.core.windows.net/lulctimeseriespublic/lc{year}/{tile_i}_{year}0101-{year+1}0101.tif\"\n",
    "        path = f\"{output_path}/{os.path.basename(tile_url)}\"\n",
    "\n",
    "        if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "        if not os.path.exists(path):\n",
    "            urllib.request.urlretrieve(tile_url, path)\n",
    "        else:\n",
    "            print(\"File already exists\")\n",
    "        landcover_downloaded.append(path)\n",
    "\n",
    "    crops = []\n",
    "    for path in landcover_downloaded:\n",
    "        src = rasterio.open(path, \"r\")\n",
    "        src_crs = src.crs\n",
    "        profile = src.profile\n",
    "        aoi_crs = aoi.to_crs(src_crs)\n",
    "        crop, transform = riomask(src, aoi_crs.geometry, all_touched=False, crop=True)\n",
    "        profile[\"width\"] = crop.shape[2]\n",
    "        profile[\"height\"] = crop.shape[1]\n",
    "        profile[\"transform\"] = transform\n",
    "        crop_name = os.path.join(\n",
    "            output_path, os.path.split(path)[1].split(\"_\")[0] + \"_crop.tif\"\n",
    "        )\n",
    "        with rasterio.open(crop_name, \"w\", **profile, nbits=1) as dst:\n",
    "            dst.write(np.where(crop == 2, 1, 0).astype(np.uint8))\n",
    "        crops.append(crop_name)\n",
    "    landcover_name = os.path.join(output_path, f\"landcover{year}.tif\")\n",
    "    listToStr = \" \".join(crops)\n",
    "    os.system(\n",
    "        \" \".join(\n",
    "            [\n",
    "                f\"gdalwarp --config GDAL_CACHEMAX 3000 -wm 3000 -t_srs {EPSG}\",\n",
    "                listToStr,\n",
    "                landcover_name,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    print(f\"{landcover_name} was merged\")\n",
    "\n",
    "\n",
    "def rescale(img, ratio):\n",
    "    width = int(img.shape[1] * ratio)\n",
    "    height = int(img.shape[0] * ratio)\n",
    "    dim = (width, height)\n",
    "    resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def mask_to_polygons(mask: np.ndarray, transform) -> MultiPolygon:\n",
    "    \"\"\"\n",
    "    Converts raster mask to shapely MultiPolygon\n",
    "    \"\"\"\n",
    "    polygons = []\n",
    "    shapes = features.shapes(\n",
    "        mask.astype(np.uint8), mask=(mask > 0), transform=transform\n",
    "    )\n",
    "\n",
    "    for shape, _ in shapes:\n",
    "        polygons.append(shapely.geometry.shape(shape))\n",
    "\n",
    "    polygons = MultiPolygon(polygons)\n",
    "    if not polygons.is_valid:\n",
    "        polygons = polygons.buffer(0)\n",
    "        if polygons.type == \"Polygon\":\n",
    "            polygons = MultiPolygon([polygons])\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def create_landcover_gdf(landcover_dir, output_dir, filename):\n",
    "    landcover_names = [\n",
    "        name for name in os.listdir(landcover_dir) if name.endswith(\"_crop.tif\")\n",
    "    ]\n",
    "\n",
    "    gdfs = []\n",
    "    for name in tqdm(landcover_names):\n",
    "        lc_fullpath = os.path.join(landcover_dir, name)\n",
    "        print(lc_fullpath)\n",
    "        with rasterio.open(lc_fullpath, \"r\") as src:\n",
    "            data = src.read().squeeze()\n",
    "            data = rescale(data, 0.5)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "            data = cv2.morphologyEx(data, cv2.MORPH_CLOSE, kernel)\n",
    "            data = cv2.morphologyEx(data, cv2.MORPH_OPEN, kernel)\n",
    "            data = rescale(data, 2)\n",
    "\n",
    "            current_polygons = list(mask_to_polygons(data, src.transform))\n",
    "            current_polygons = [poly for poly in current_polygons if poly.area]\n",
    "\n",
    "            current_areas = [poly.area for poly in current_polygons]\n",
    "            current_tilename = [name] * len(current_areas)\n",
    "\n",
    "            crs = src.crs\n",
    "            current_gdf = gpd.GeoDataFrame(\n",
    "                {\n",
    "                    \"area\": current_areas,\n",
    "                    \"names\": current_tilename,\n",
    "                    \"geometry\": current_polygons,\n",
    "                },\n",
    "                crs=crs,\n",
    "            )\n",
    "            current_gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "            gdfs.append(current_gdf.copy())\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=\"EPSG:4326\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"{filename}.geojson\")\n",
    "    gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def prepare_landcover(year, landcover_tiles, output_path, aoi):\n",
    "    filename = f\"{landcover_tiles[0]}_{year}_landcover\"\n",
    "\n",
    "    for file in os.listdir(output_path):\n",
    "        if file != LANDCOVER_FILENAME and not file.endswith(\"_landcover.geojson\"):\n",
    "            os.remove(os.path.join(output_path, file))\n",
    "\n",
    "    for file in os.listdir(output_path):\n",
    "        if filename in file:\n",
    "            return os.path.join(output_path, file)\n",
    "\n",
    "    landcover_annual(year, landcover_tiles, output_path, aoi)\n",
    "    landcover_geojson = create_landcover_gdf(output_path, output_path, filename)\n",
    "    return landcover_geojson\n",
    "\n",
    "\n",
    "def weights_exists_or_download(path, file_id):\n",
    "    if not Path(path).exists():\n",
    "        creds_file = os.environ.get(\"CREDENTIAL_FILE\")\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            creds_file, scopes=SCOPES\n",
    "        )\n",
    "\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "\n",
    "        fh = io.FileIO(\"unet_v4.pth\", mode=\"wb\")\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Download {int(status.progress() * 100)}\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cfe616",
   "metadata": {},
   "source": [
    "### 4. Unet-diff inference on the prepared data and then postprocessing on cloud and forest landcover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41fa8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUDS_PROBABILITY_THRESHOLD = 15\n",
    "NEAREST_POLYGONS_NUMBER = 10\n",
    "DATES_FOR_TILE = 2\n",
    "\n",
    "\n",
    "os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(message)s\")\n",
    "\n",
    "\n",
    "def predict(image_tensor, model, channels, neighbours, size, device):\n",
    "    image_shape = 1, count_channels(channels) * neighbours, size, size\n",
    "    prediction, _ = model.predict(\n",
    "        image_tensor.view(image_shape).to(device, dtype=torch.float)\n",
    "    )\n",
    "    result = prediction.view(size, size).detach().cpu().numpy()\n",
    "    return result\n",
    "\n",
    "\n",
    "def diff(img1, img2):\n",
    "    img2 = match_histograms(img2, img1, multichannel=True)\n",
    "    difference = (img1 - img2) / (img1 + img2)\n",
    "    difference = (difference + 1) * 127\n",
    "    return np.concatenate(\n",
    "        (difference.astype(np.uint8), img1.astype(np.uint8), img2.astype(np.uint8)),\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def mask_postprocess(mask):\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    erosion = cv2.erode(mask, kernel, iterations=1)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    closing = cv2.morphologyEx(erosion, cv2.MORPH_CLOSE, kernel)\n",
    "    return closing\n",
    "\n",
    "\n",
    "def predict_raster(\n",
    "    img_current,\n",
    "    img_previous,\n",
    "    channels,\n",
    "    network=\"unet-diff\",\n",
    "    model_weights_path=\"/code/models/unet_diff.pth\",\n",
    "    input_size=56,\n",
    "    neighbours=3,\n",
    "):\n",
    "    model, device = load_model(network, model_weights_path)\n",
    "\n",
    "    with rasterio.open(img_current) as source_current, rasterio.open(\n",
    "        img_previous\n",
    "    ) as source_previous:\n",
    "        meta = source_current.meta\n",
    "        meta[\"count\"] = 1\n",
    "\n",
    "        clearcut_mask = np.zeros((source_current.height, source_current.width))\n",
    "        for i in tqdm(range(source_current.width // input_size)):\n",
    "            for j in range(source_current.height // input_size):\n",
    "                bottom_row = j * input_size\n",
    "                upper_row = (j + 1) * input_size\n",
    "                left_column = i * input_size\n",
    "                right_column = (i + 1) * input_size\n",
    "\n",
    "                corners = [\n",
    "                    source_current.xy(bottom_row, left_column),\n",
    "                    source_current.xy(bottom_row, right_column),\n",
    "                    source_current.xy(upper_row, right_column),\n",
    "                    source_current.xy(upper_row, left_column),\n",
    "                    source_current.xy(bottom_row, left_column),\n",
    "                ]\n",
    "\n",
    "                window = Window(bottom_row, left_column, input_size, input_size)\n",
    "                image_current = reshape_as_image(source_current.read(window=window))\n",
    "                image_previous = reshape_as_image(source_previous.read(window=window))\n",
    "\n",
    "                difference_image = diff(image_current, image_previous)\n",
    "                image_tensor = transforms.ToTensor()(\n",
    "                    difference_image.astype(np.uint8)\n",
    "                ).to(device, dtype=torch.float)\n",
    "\n",
    "                predicted = predict(\n",
    "                    image_tensor, model, channels, neighbours, input_size, device\n",
    "                )\n",
    "                predicted = mask_postprocess(predicted)\n",
    "                clearcut_mask[\n",
    "                    left_column:right_column, bottom_row:upper_row\n",
    "                ] += predicted\n",
    "    meta[\"dtype\"] = \"float32\"\n",
    "    return clearcut_mask.astype(np.float32), meta\n",
    "\n",
    "\n",
    "def count_channels(channels):\n",
    "    count = 0\n",
    "    for ch in channels:\n",
    "        ch = ch.lower()\n",
    "        if ch == \"rgb\":\n",
    "            count += 3\n",
    "        elif ch in [\"ndvi\", \"ndmi\", \"b08\", \"b8a\", \"b11\", \"b12\"]:\n",
    "            count += 1\n",
    "        else:\n",
    "            raise Exception(\"{} channel is unknown!\".format(ch))\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def scale(tensor, max_value):\n",
    "    max_ = tensor.max()\n",
    "    if max_ > 0:\n",
    "        return tensor / max_ * max_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def save_raster(raster_array, meta, save_path, filename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        logging.info(\"Data directory created.\")\n",
    "\n",
    "    filename = filename.split(\"_all_merged\")[-1]\n",
    "    save_path = os.path.join(save_path, f\"predicted_{filename}\")\n",
    "\n",
    "    cv2.imwrite(f\"{save_path}.png\", raster_array)\n",
    "\n",
    "    with rasterio.open(f\"{save_path}.tif\", \"w\", **meta) as dst:\n",
    "        for i in range(1, meta[\"count\"] + 1):\n",
    "            dst.write(raster_array, i)\n",
    "\n",
    "\n",
    "def polygonize(raster_array, meta, transform=True, mode=cv2.RETR_TREE):\n",
    "    raster_array = (raster_array * 255).astype(np.uint8)\n",
    "\n",
    "    contours, _ = cv2.findContours(raster_array, mode, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    polygons = []\n",
    "    for i in tqdm(range(len(contours))):\n",
    "        c = contours[i]\n",
    "        n_s = (c.shape[0], c.shape[2])\n",
    "        if n_s[0] > 2:\n",
    "            if transform:\n",
    "                polys = [tuple(i) * meta[\"transform\"] for i in c.reshape(n_s)]\n",
    "            else:\n",
    "                polys = [tuple(i) for i in c.reshape(n_s)]\n",
    "            polygons.append(Polygon(polys))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def save_polygons(polygons, save_path, filename):\n",
    "    if len(polygons) == 0:\n",
    "        logging.info(\"no_polygons detected\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        logging.info(\"Data directory created.\")\n",
    "\n",
    "    logging.info(f\"{filename} saved.\")\n",
    "    print(f\"{filename} saved.\")\n",
    "    polygons.to_file(os.path.join(save_path, f\"{filename}.geojson\"), driver=\"GeoJSON\")\n",
    "\n",
    "\n",
    "def intersection_poly(test_poly, mask_poly):\n",
    "    intersecion_score = False\n",
    "    if test_poly.is_valid and mask_poly.is_valid:\n",
    "        intersection_result = test_poly.intersection(mask_poly)\n",
    "        if not intersection_result.is_valid:\n",
    "            intersection_result = intersection_result.buffer(0)\n",
    "        if not intersection_result.is_empty:\n",
    "            intersecion_score = True\n",
    "    return intersecion_score\n",
    "\n",
    "\n",
    "def morphological_transform(img):\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    closing = cv2.dilate(closing, kernel, iterations=1)\n",
    "    return closing\n",
    "\n",
    "\n",
    "def postprocessing(tile, cloud_files, clearcuts, src_crs, year, aoi):\n",
    "    def get_intersected_polygons(polygons, masks, mask_column_name):\n",
    "        \"\"\"Finding in GeoDataFrame with clearcuts the masked polygons.\n",
    "\n",
    "        :param polygons: GeoDataFrame with clearcuts and mask columns\n",
    "        :param masks: list of masks (e.g., polygons of clouds)\n",
    "        :param mask_column_name: name of mask column in polygons GeoDataFrame\n",
    "\n",
    "        :return: GeoDataFrame with filled mask flags in corresponding column\n",
    "        \"\"\"\n",
    "        masked_values = []\n",
    "        if len(masks) > 0:\n",
    "            centroids = [[mask.centroid.x, mask.centroid.y] for mask in masks]\n",
    "            kdtree = spatial.KDTree(centroids)\n",
    "            for _, clearcut in polygons.iterrows():\n",
    "                polygon = clearcut[\"geometry\"]\n",
    "                _, idxs = kdtree.query(polygon.centroid, k=NEAREST_POLYGONS_NUMBER)\n",
    "                masked_value = 0\n",
    "                for idx in idxs:\n",
    "                    if idx >= len(masks):\n",
    "                        break\n",
    "                    if intersection_poly(polygon, masks[idx].buffer(0)):\n",
    "                        masked_value = 1\n",
    "                        break\n",
    "                masked_values.append(masked_value)\n",
    "        polygons[mask_column_name] = masked_values\n",
    "        return polygons\n",
    "\n",
    "    landcover = LandcoverPolygons(tile, src_crs, year, aoi)\n",
    "    forest_polygons = landcover.get_polygon()\n",
    "\n",
    "    #     cloud_files = [f\"{img_path}/{tile}_{i}/clouds.tiff\" for i in range(DATES_FOR_TILE)]\n",
    "    cloud_polygons = []\n",
    "    for cloud_file in cloud_files:\n",
    "        with rasterio.open(cloud_file) as src:\n",
    "            clouds = src.read(1)\n",
    "            meta = src.meta\n",
    "        clouds = morphological_transform(clouds)\n",
    "        clouds = (clouds > CLOUDS_PROBABILITY_THRESHOLD).astype(np.uint8)\n",
    "        if clouds.sum() > 0:\n",
    "            cloud_polygons.extend(polygonize(clouds, meta, mode=cv2.RETR_LIST))\n",
    "\n",
    "    n_clearcuts = len(clearcuts)\n",
    "    polygons = {\n",
    "        \"geometry\": clearcuts,\n",
    "        \"forest\": np.zeros(n_clearcuts),\n",
    "        \"clouds\": np.zeros(n_clearcuts),\n",
    "    }\n",
    "\n",
    "    polygons = geopandas.GeoDataFrame(polygons, crs=src_crs)\n",
    "\n",
    "    if len(cloud_polygons) > 0:\n",
    "        polygons = get_intersected_polygons(polygons, cloud_polygons, \"clouds\")\n",
    "    else:\n",
    "        print('Clouds with specified CLOUDS_PROBABILITY_THRESHOLD not found')\n",
    "    polygons = get_intersected_polygons(polygons, forest_polygons, \"forest\")\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56fd33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deforestation_inference(\n",
    "    img_start_path,\n",
    "    img_end_path,\n",
    "    tile,\n",
    "    network=\"unet-diff\",\n",
    "    model_weights_path=MODEL_PATH,\n",
    "    save_path=OUTPUT_FOLDER,\n",
    "    channels=[\"RGB\", \"B08\", \"B8A\", \"B11\", \"B12\", \"NDVI\", \"NDMI\"],\n",
    "    threshold=0.4,\n",
    "    polygonize_only=False,\n",
    "):\n",
    "    filename = img_start_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    predicted_filename = f\"predicted_{filename}\"\n",
    "\n",
    "    if not polygonize_only:\n",
    "        raster_array, meta = predict_raster(\n",
    "            img_start_path, img_end_path, channels, network, model_weights_path\n",
    "        )\n",
    "        save_raster(raster_array, meta, save_path, filename)\n",
    "    else:\n",
    "        with rasterio.open(os.path.join(save_path, f\"{predicted_filename}.tif\")) as src:\n",
    "            raster_array = src.read()\n",
    "            raster_array = np.moveaxis(raster_array, 0, -1)\n",
    "            meta = src.meta\n",
    "            src.close()\n",
    "\n",
    "    logging.info(\"Polygonize raster array of clearcuts...\")\n",
    "    clearcuts = polygonize(raster_array > threshold, meta)\n",
    "    logging.info(\"Filter polygons of clearcuts\")\n",
    "    polygons = postprocessing(\n",
    "        tile,\n",
    "        [\n",
    "            os.path.join(CLOUD_DATA_FOLDER, file)\n",
    "            for file in os.listdir(CLOUD_DATA_FOLDER)\n",
    "        ],\n",
    "        clearcuts,\n",
    "        meta[\"crs\"],\n",
    "        year,\n",
    "        aoi,\n",
    "    )\n",
    "\n",
    "    save_polygons(polygons, save_path, predicted_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68a661e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [18:04<00:00,  5.53s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4243/4243 [00:00<00:00, 12437.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying color table from /code/data/landcovers/16T_crop.tif to new file.\n",
      "Creating output file that is 1853P x 1060L.\n",
      "Processing input file /code/data/landcovers/16T_crop.tif.\n",
      "Using internal nodata values (e.g. 0) for image /code/data/landcovers/16T_crop.tif.\n",
      "Copying nodata values from source /code/data/landcovers/16T_crop.tif to destination /code/data/landcovers/landcover2020.tif.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "/code/data/landcovers/landcover2020.tif was merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code/data/landcovers/16T_crop.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_all_merged_S2B_MSIL1C_20200424T161829_N0209_R040_T16TFK_20200424T195711 saved.\n"
     ]
    }
   ],
   "source": [
    "deforestation_inference(start_date_merged_path, end_date_merged_path, tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9295ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04440ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
